Perfect â€” hereâ€™s an **updated, realistic, presentation-ready README.md** for your current project setup.

This version:
âœ… Matches your **actual code flow** (no separate plotting file/folder)
âœ… Lists only your **used metrics**: Recall@K, Precision@K, BLEU, ROUGE-L
âœ… Includes an **ASCII-style architecture flow diagram** (so it looks great even on GitHub)
âœ… Keeps the structure clean and professional

---

# ğŸ§  Multi-Turn RAG (Retrieval-Augmented Generation) Evaluator

This project implements a **multi-turn RAG evaluation workflow**, combining **retrieval** and **generation** modules to assess how effectively a language model can generate factual, context-grounded answers.

The pipeline performs:

* Passage **retrieval** using **FAISS** or **Chroma**
* Text **generation** using **Gemini API**
* Metric-based **evaluation** (Recall@K, Precision@K, BLEU, ROUGE-L)

It includes 3 evaluation tasks covering both retrieval and generation aspects of RAG systems.

---

## ğŸ“‹ Table of Contents

1. [Overview](#overview)
2. [Architecture Flow](#architecture-flow)
3. [Project Structure](#project-structure)
4. [Tasks](#tasks)
5. [Setup & Installation](#setup--installation)
6. [Running the Workflow](#running-the-workflow)
7. [Evaluation Metrics](#evaluation-metrics)
8. [Outputs](#outputs)
9. [Example Run](#example-run)
10. [Future Work](#future-work)

---

## ğŸ§© Overview

**Retrieval-Augmented Generation (RAG)** enhances LLMs by grounding their answers in external documents.
In **multi-turn** settings, user queries depend on previous context, requiring strong retrieval and reasoning.

This project:

* Builds and indexes a text corpus
* Retrieves relevant passages per query
* Generates factual responses using Gemini
* Evaluates performance using objective metrics

---

## âš™ï¸ Architecture Flow

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚      Input Queries          â”‚
                 â”‚  (multi-turn conversations) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Corpus Preparation      â”‚
                 â”‚ (clean JSONL -> corpus)   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    Embedding + Indexing   â”‚
                 â”‚ (FAISS / Chroma Index)    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Retrieval (Task A)      â”‚
                 â”‚ Return top-k passages     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Generation                 â”‚
                 â”‚  Task B: Gold references    â”‚
                 â”‚  Task C: Retrieved passages â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Evaluation Metrics      â”‚
                 â”‚ Recall@K, Precision@K,    â”‚
                 â”‚ BLEU, ROUGE-L             â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  Printed Results Summary  â”‚
                 â”‚  + Graph (if applicable)  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Project Structure

```
MTRagEval/
â”‚
â”œâ”€â”€ data/                          # Raw source datasets
â”‚   â”œâ”€â”€ cloud.jsonl
â”‚   â”œâ”€â”€ fiqa.jsonl
â”‚   â””â”€â”€ govt.jsonl
â”‚
â”œâ”€â”€ data_processed/                # Cleaned / merged corpus
â”‚   â”œâ”€â”€ corpus.jsonl
â”‚   â””â”€â”€ corpus copy.jsonl
â”‚
â”œâ”€â”€ index_faiss/                   # FAISS index and metadata
â”‚   â”œâ”€â”€ corpus.index
â”‚   â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ src/                           # Core project source code
â”‚   â”œâ”€â”€ build_index.py             # Builds vector index from processed corpus
â”‚   â”œâ”€â”€ generation_referenced.py   # Task B: Generation using gold references
â”‚   â”œâ”€â”€ generation_retrieved.py    # Task C: Generation using retrieved passages (RAG)
â”‚   â”œâ”€â”€ ingest_and_clean.py        # Cleans and preprocesses raw data
â”‚   â”œâ”€â”€ retrieval.py               # Task A: Retrieval logic (FAISS or Chroma)
â”‚   â””â”€â”€ summarizer.py              # Summarizes long retrieved texts
â”‚
â”œâ”€â”€ .env                           # Environment variables (Gemini API key)
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”œâ”€â”€ data.txt                       # Text file used as reference context
â”œâ”€â”€ evaluation.py                  # Metric computations (Recall, Precision, BLEU, ROUGE)
â”œâ”€â”€ LICENSE                        # License info
â”œâ”€â”€ main.py                        # Main script to run Tasks A, B, and C + metrics plotting
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ test.py                        # Script for debugging or data inspection

```

---

## ğŸ§  Tasks

| **Task** | **Name**               | **Goal**                                   | **Input**             | **Output**      |
| -------- | ---------------------- | ------------------------------------------ | --------------------- | --------------- |
| **A**    | Retrieval              | Retrieve top-K most relevant passages      | Query                 | Ranked passages |
| **B**    | Generation (Reference) | Generate using **gold** reference passages | Query + Gold passages | Generated text  |
| **C**    | Generation (RAG)       | Full retrieval + generation pipeline       | Query only            | Generated text  |

---

## ğŸ§° Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/Niall1985/MTRagEval.git
cd MTRAG-Evaluator
```

### 2ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate  # Windows
# or
source venv/bin/activate  # Mac/Linux
```

### 3ï¸âƒ£ Install Requirements

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add Your Gemini API Key

Create a `.env` file in the root:

```
gemini_api=YOUR_GEMINI_API_KEY
```

---

## â–¶ï¸ Running the Workflow

### Step 1 â€” Prepare Corpus

Ensure you have a valid `corpus.jsonl` file:

```bash
python ingest_and_clean.py
```

### Step 2 â€” Build Index

You can use **FAISS** or **Chroma**.

```bash
python build_index.py
```

### Step 3 â€” Run Evaluation

```bash
python main.py
```

This will:

* Perform retrieval (Task A)
* Generate using references (Task B)
* Run full RAG generation (Task C)
* Compute all metrics and print them to the console

---

## ğŸ“Š Evaluation Metrics

| **Metric**      | **Type**   | **Meaning**                                                |
| --------------- | ---------- | ---------------------------------------------------------- |
| **Recall@K**    | Retrieval  | Fraction of relevant docs retrieved in top-K               |
| **Precision@K** | Retrieval  | Fraction of retrieved docs that are relevant               |
| **BLEU-4**      | Generation | N-gram overlap between generated and reference text        |
| **ROUGE-L**     | Generation | Longest common subsequence between generated and reference |

---

## ğŸ“¤ Outputs

After execution, you will see printed outputs such as:

```
Retrieval Evaluation:
  Recall@5: 0.82
  Precision@5: 0.74

Generation Evaluation (Reference):
  BLEU-4: 0.68
  ROUGE-L: 0.72

Generation Evaluation (RAG):
  BLEU-4: 0.63
  ROUGE-L: 0.69
```

If graph plotting is enabled in your code (e.g. via `matplotlib`), it will display inline â€” no separate `plots/` folder is needed.

---

## ğŸ§ª Example Run

**User Query:**

> â€œWhich California state parks allow dogs on the beach?â€

**Task A (Retrieval)**
â†’ Returns top 5 passages from the corpus related to dog-friendly beaches.

**Task B (Generation w/ Reference)**

> â€œDogs are allowed in most California state parks if kept on a leash not exceeding six feet...â€

**Task C (Full RAG)**

> â€œSome California state parks like Huntington and Fort Funston allow dogs in designated beach areas when leashed.â€

---

## ğŸš€ Future Work

* Add **nDCG** and **MRR** for deeper retrieval analysis
* Integrate **reranking** models like `bge-reranker`
* Extend to **multilingual corpora**
* Add **human evaluation interface**
* Compare Gemini with **OpenAI GPT or Mistral models**

---

## ğŸ‘¤ Author

**Niall Dâ€™cunha**
RAG Evaluation Workflow Developer
